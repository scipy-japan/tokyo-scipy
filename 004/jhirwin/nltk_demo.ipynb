{
 "metadata": {
  "name": "Kan.Scipy #1"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "source": [
      "# Kan.Scipy #1", 
      "", 
      "Welcome to Kan.Scipy #1!", 
      "", 
      "## Introducing NLTK", 
      "", 
      "### First, some imports:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "import nltk", 
      "import MeCab", 
      "from nltk import Tree", 
      "from nltk.corpus import brown, gutenberg, treebank", 
      "from nltk.tokenize.api import TokenizerI"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 19
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Corpora", 
      "", 
      "NLTK has several built-in corpora and resources"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "treebank.sents()"
     ], 
     "language": "python", 
     "outputs": [
      {
       "ename": "LookupError", 
       "evalue": "\n**********************************************************************\n  Resource 'corpora/treebank/combined' not found.  Please use the\n  NLTK Downloader to obtain the resource: >>> nltk.download().\n  Searched in:\n    - '/home/joseph/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************", 
       "output_type": "pyerr", 
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)", 
        "\u001b[0;32m/home/joseph/Projects/kanscipy/<ipython-input-6-508d6b7c8d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtreebank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0m\n", 
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0m\n", 
        "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/treebank/combined' not found.  Please use the\n  NLTK Downloader to obtain the resource: >>> nltk.download().\n  Searched in:\n    - '/home/joseph/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
       ]
      }
     ], 
     "prompt_number": 6
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "nltk.download('treebank')"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "[nltk_data] Downloading package 'treebank' to", 
        "[nltk_data]     /home/joseph/nltk_data...", 
        "[nltk_data]   Unzipping corpora/treebank.zip."
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        ""
       ]
      }, 
      {
       "output_type": "pyout", 
       "prompt_number": 7, 
       "text": [
        "True"
       ]
      }
     ], 
     "prompt_number": 7
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "print treebank.parsed_sents()[0]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "(S", 
        "  (NP-SBJ", 
        "    (NP (NNP Pierre) (NNP Vinken))", 
        "    (, ,)", 
        "    (ADJP (NP (CD 61) (NNS years)) (JJ old))", 
        "    (, ,))", 
        "  (VP", 
        "    (MD will)", 
        "    (VP", 
        "      (VB join)", 
        "      (NP (DT the) (NN board))", 
        "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))", 
        "      (NP-TMP (NNP Nov.) (CD 29))))", 
        "  (. .))"
       ]
      }
     ], 
     "prompt_number": 10
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "NLTK's CorpusReader classes manage files:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "print brown.abspaths()[:5]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "[FileSystemPathPointer('/home/joseph/nltk_data/corpora/brown/ca01'), FileSystemPathPointer('/home/joseph/nltk_data/corpora/brown/ca02'), FileSystemPathPointer('/home/joseph/nltk_data/corpora/brown/ca03'), FileSystemPathPointer('/home/joseph/nltk_data/corpora/brown/ca04'), FileSystemPathPointer('/home/joseph/nltk_data/corpora/brown/ca05')]"
       ]
      }
     ], 
     "prompt_number": 31
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "", 
      "### Tokenization", 
      "", 
      "NLTK has a built-in tokenizer for English:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "text = 'The quick brown fox jumped over the lazy dog.'", 
      "nltk.word_tokenize(text)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 11, 
       "text": [
        "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']"
       ]
      }
     ], 
     "prompt_number": 11
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "#### For Japanese:", 
      "", 
      "You can call MeCab from Python:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "jtext = u'\u3059\u3070\u3057\u3063\u3053\u3044\u8336\u8272\u306e\u72d0\u304c\u6020\u3051\u8005\u306e\u72ac\u306e\u4e0a\u3092\u98db\u3093\u3067\u3044\u3063\u305f\u3068\u3055\u3002'", 
      "mecab = MeCab.Tagger()", 
      "print mecab.parse(jtext.encode('euc-jp')).decode('euc-jp')"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "\u3059\t\u63a5\u982d\u8f9e,\u540d\u8a5e\u63a5\u982d\u8f9e,*,*,\u3059,\u3059,*", 
        "\u3070\t\u540d\u8a5e,\u666e\u901a\u540d\u8a5e,*,*,\u3070,\u3070,\u6f22\u5b57\u8aad\u307f:\u8a13 \u4ee3\u8868\u8868\u8a18:\u5834", 
        "\u3057\u3063\t\u52d5\u8a5e,*,\u30b5\u5909\u52d5\u8a5e,\u8a9e\u5e79,\u3057\u3063\u3059\u308b,\u3057\u3063,\u4ee3\u8868\u8868\u8a18:\u5931\u3059\u308b", 
        "\u3053\u3044\t\u5f62\u5bb9\u8a5e,*,\u30a4\u5f62\u5bb9\u8a5e\u30a2\u30a6\u30aa\u6bb5,\u57fa\u672c\u5f62,\u3053\u3044,\u3053\u3044,\u4ee3\u8868\u8868\u8a18:\u6fc3\u3044", 
        "\u8336\u8272\t\u540d\u8a5e,\u666e\u901a\u540d\u8a5e,*,*,\u8336\u8272,\u3061\u3083\u3044\u308d,\u4ee3\u8868\u8868\u8a18:\u8336\u8272", 
        "\u306e\t\u52a9\u8a5e,\u63a5\u7d9a\u52a9\u8a5e,*,*,\u306e,\u306e,*", 
        "\u72d0\t\u540d\u8a5e,\u666e\u901a\u540d\u8a5e,*,*,\u72d0,\u304d\u3064\u306d,\u4ee3\u8868\u8868\u8a18:\u72d0", 
        "\u304c\t\u52a9\u8a5e,\u683c\u52a9\u8a5e,*,*,\u304c,\u304c,*", 
        "\u6020\u3051\u8005\t\u540d\u8a5e,\u666e\u901a\u540d\u8a5e,*,*,\u6020\u3051\u8005,\u306a\u307e\u3051\u3082\u306e,\u4ee3\u8868\u8868\u8a18:\u6020\u3051\u8005", 
        "\u306e\t\u52a9\u8a5e,\u63a5\u7d9a\u52a9\u8a5e,*,*,\u306e,\u306e,*", 
        "\u72ac\t\u540d\u8a5e,\u666e\u901a\u540d\u8a5e,*,*,\u72ac,\u3044\u306c,\u6f22\u5b57\u8aad\u307f:\u8a13 \u4ee3\u8868\u8868\u8a18:\u72ac", 
        "\u306e\t\u52a9\u8a5e,\u63a5\u7d9a\u52a9\u8a5e,*,*,\u306e,\u306e,*", 
        "\u4e0a\t\u540d\u8a5e,\u526f\u8a5e\u7684\u540d\u8a5e,*,*,\u4e0a,\u3046\u3048,*", 
        "\u3092\t\u52a9\u8a5e,\u683c\u52a9\u8a5e,*,*,\u3092,\u3092,*", 
        "\u98db\u3093\u3067\t\u52d5\u8a5e,*,\u5b50\u97f3\u52d5\u8a5e\u30d0\u884c,\u30bf\u7cfb\u9023\u7528\u30c6\u5f62,\u98db\u3076,\u3068\u3093\u3067,\u4ee3\u8868\u8868\u8a18:\u98db\u3076", 
        "\u3044\u3063\u305f\t\u63a5\u5c3e\u8f9e,\u52d5\u8a5e\u6027\u63a5\u5c3e\u8f9e,\u5b50\u97f3\u52d5\u8a5e\u30ab\u884c\u4fc3\u97f3\u4fbf\u5f62,\u30bf\u5f62,\u3044\u304f,\u3044\u3063\u305f,*", 
        "\u3068\t\u52a9\u8a5e,\u683c\u52a9\u8a5e,*,*,\u3068,\u3068,*", 
        "\u3055\t\u52a9\u8a5e,\u7d42\u52a9\u8a5e,*,*,\u3055,\u3055,*", 
        "\u3002\t\u7279\u6b8a,\u53e5\u70b9,*,*,\u3002,\u3002,*", 
        "EOS", 
        ""
       ]
      }
     ], 
     "prompt_number": 17
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Or define a new NLTK tokenizer using MeCab: (code copied from `https://mhagiwara.googlecode.com/svn/trunk/nltk/jpbook/jptokenizer.py` )"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "class JPMeCabTokenizer(TokenizerI):", 
      "    def __init__(self):", 
      "        import MeCab", 
      "        self.mecab = MeCab.Tagger('-Owakati')", 
      "", 
      "    def tokenize(self, text):", 
      "        result = self.mecab.parse(text.encode('euc-jp'))", 
      "        return result.decode('euc-jp').strip().split(' ')", 
      "", 
      "print JPMeCabTokenizer().tokenize(jtext)", 
      "print u' '.join(JPMeCabTokenizer().tokenize(jtext))"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "[u'\\u3059', u'\\u3070', u'\\u3057\\u3063', u'\\u3053\\u3044', u'\\u8336\\u8272', u'\\u306e', u'\\u72d0', u'\\u304c', u'\\u6020\\u3051\\u8005', u'\\u306e', u'\\u72ac', u'\\u306e', u'\\u4e0a', u'\\u3092', u'\\u98db\\u3093\\u3067', u'\\u3044\\u3063\\u305f', u'\\u3068', u'\\u3055', u'\\u3002']", 
        "\u3059 \u3070 \u3057\u3063 \u3053\u3044 \u8336\u8272 \u306e \u72d0 \u304c \u6020\u3051\u8005 \u306e \u72ac \u306e \u4e0a \u3092 \u98db\u3093\u3067 \u3044\u3063\u305f \u3068 \u3055 \u3002"
       ]
      }
     ], 
     "prompt_number": 24
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Ngram Language Models", 
      "", 
      "NLTK provides functionality to build n-gram language models.", 
      "", 
      "A language model is a probabilistic model of language that allows", 
      "us to measure how likely a given sequence of words is.", 
      "", 
      "An n-gram is a sequence of n words; we count n-grams in a text and", 
      "calculate a conditional probability distribution like:", 
      "", 
      "$$", 
      "P(X_i|X_{i-1},..,X_{i-n+1})", 
      "$$"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "from nltk.model.ngram import NgramModel", 
      "from nltk.probability import WittenBellProbDist, LidstoneProbDist", 
      "", 
      "train_words = brown.words()[:-500]", 
      "test_words = brown.words()[-500:]", 
      "lm = NgramModel(2, train_words, lambda fd, b: LidstoneProbDist(fd, 0.2))"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 37
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "lm.entropy(test_words)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "ename": "TypeError", 
       "evalue": "not all arguments converted during string formatting", 
       "output_type": "pyerr", 
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)", 
        "\u001b[0;32m/home/joseph/Projects/kanscipy/<ipython-input-38-a41416bdebe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/nltk/model/ngram.pyc\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/nltk/model/ngram.pyc\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_random_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/nltk/model/ngram.pyc\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backoff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise RuntimeError(\"No probability mass assigned to word %s in \" +\n", 
        "\u001b[0;32m/usr/lib/python2.7/dist-packages/nltk/model/ngram.pyc\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             raise RuntimeError(\"No probability mass assigned to word %s in \" +\n\u001b[0;32m---> 82\u001b[0;31m                                \"context %s\" % (word, ' '.join(context)))\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
        "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
       ]
      }
     ], 
     "prompt_number": 38
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Counting", 
      "", 
      "For example, how many words in a corpus are not in WordNet?"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "from nltk.corpus import wordnet", 
      "from nltk.probability import ConditionalFreqDist", 
      "", 
      "cfd = ConditionalFreqDist(", 
      "      (pos, len(wordnet.synsets(word)) > 0) for word,pos in treebank.tagged_words()", 
      ")", 
      "", 
      "cfd.tabulate()"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "       False True", 
        "     #   16    0", 
        "     $  724    0", 
        "    ''  694    0", 
        "     , 4885    1", 
        " -LRB-  120    0", 
        "-NONE- 5493 1099", 
        " -RRB-  126    0", 
        "     . 3874    0", 
        "     :  563    0", 
        "    CC 1651  614", 
        "    CD 1527 2019", 
        "    DT 5230 2935", 
        "    EX    0   88", 
        "    FW    1    3", 
        "    IN 5354 4503", 
        "    JJ  676 5158", 
        "   JJR    1  380", 
        "   JJS    1  181", 
        "    LS    0   13", 
        "    MD  409  518", 
        "    NN  785 12381", 
        "   NNP 2770 6640", 
        "  NNPS    7  237", 
        "   NNS   93 5954", 
        "   PDT    0   27", 
        "   POS  824    0", 
        "   PRP  686 1030", 
        "  PRP$  423  343", 
        "    RB  343 2479", 
        "   RBR    0  136", 
        "   RBS    0   35", 
        "    RP    1  215", 
        "   SYM    1    0", 
        "    TO 2179    0", 
        "    UH    0    3", 
        "    VB    4 2550", 
        "   VBD    1 3042", 
        "   VBG    2 1458", 
        "   VBN    5 2129", 
        "   VBP   59 1262", 
        "   VBZ  104 2021", 
        "   WDT  445    0", 
        "    WP   74  167", 
        "   WP$   14    0", 
        "   WRB  164   14", 
        "    ``  712    0"
       ]
      }
     ], 
     "prompt_number": 41
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Missing functionality", 
      "", 
      "#### Head word identification", 
      "", 
      "NLTK has no functionality to identify the head words of phrases. In this noun phrase, 'man' is the head word,", 
      "but it is not straightforward to identify it."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "np = Tree('(NP (D The) (N man) (PP (P with) (NP (D a) (N gun))))')", 
      "np.draw()"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 58
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "# Last words:", 
      "", 
      "## A tip", 
      "", 
      "Did you know you can add arbitrary attributes to an object instance?"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "class MyClass: pass", 
      "", 
      "mc = MyClass()", 
      "", 
      "mc.foo = 'bar'", 
      "", 
      "print mc.foo"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "bar"
       ]
      }
     ], 
     "prompt_number": 49
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "This is useful for dynamic programming, but how do you test for presence/abscence?"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "print mc.baz is None"
     ], 
     "language": "python", 
     "outputs": [
      {
       "ename": "AttributeError", 
       "evalue": "MyClass instance has no attribute 'baz'", 
       "output_type": "pyerr", 
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", 
        "\u001b[0;32m/home/joseph/Projects/kanscipy/<ipython-input-54-63eaf6b88ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaz\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
        "\u001b[0;31mAttributeError\u001b[0m: MyClass instance has no attribute 'baz'"
       ]
      }
     ], 
     "prompt_number": 54
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "print hasattr(mc, 'baz')"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "False"
       ]
      }
     ], 
     "prompt_number": 57
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "But `hasattr` is controversial and may disappear"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "try:", 
      "    print mc.baz is None", 
      "except AttributeError:", 
      "    pass"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 55
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "def tryattr(obj, attr, default=None):", 
      "    try:", 
      "        return getattr(obj, attr)", 
      "    except AttributeError:", 
      "        return default", 
      "", 
      "print tryattr(mc, 'baz')", 
      "print tryattr(mc, 'foo')"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "None", 
        "bar"
       ]
      }
     ], 
     "prompt_number": 56
    }
   ]
  }
 ]
}